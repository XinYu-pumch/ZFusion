#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
All-in-One Workflow: LoRA Fix -> Merge -> Quantize to GGUF

æ•´åˆå·¥ä½œæµï¼š
1. (å¯é€‰) ä¿®å¤ ai-toolkit ç”Ÿæˆçš„ LoRA (Zimage/Lumina2 æ ¼å¼è½¬æ¢)ã€‚
2. å°† LoRA ä¸åŸºç¡€æ¨¡å‹èåˆã€‚
3. å°†èåˆåçš„æ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼å¹¶è¿›è¡Œé‡åŒ–ã€‚

ä½œè€…: æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸‰ä¸ªè„šæœ¬æ•´åˆè€Œæˆ
æ—¥æœŸ: 2024-05-22
"""

import os
import sys
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import threading
import traceback
import logging
import argparse
import numpy as np
from collections import defaultdict
from pathlib import Path
import tempfile
from tqdm import tqdm

# --- ä¾èµ–æ£€æŸ¥ ---
try:
    import torch
    from safetensors.torch import load_file, save_file
    from safetensors import safe_open
    import gguf
    from gguf import GGUFWriter, GGMLQuantizationType, LlamaFileType
except ImportError as e:
    # åœ¨GUIå¯åŠ¨å‰è¿›è¡Œæ£€æŸ¥ï¼Œå¦‚æœå¤±è´¥åˆ™æ— æ³•å¯åŠ¨
    print(f"é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„åº“ã€‚è¯·å…ˆå®‰è£…ä¾èµ–ï¼š\n{e}")
    print("\nè¯·åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“:")
    print("pip install torch safetensors gguf numpy tqdm")
    sys.exit(1)

# --- å…¨å±€æ—¥å¿—è®°å½•å™¨ ---
class GuiLogger:
    def __init__(self, text_widget):
        self.text_widget = text_widget
        self.text_widget.config(state='disabled')

    def log(self, msg, level="INFO"):
        self.text_widget.config(state='normal')
        self.text_widget.insert(tk.END, f"[{level}] {msg}\n")
        self.text_widget.see(tk.END)
        self.text_widget.config(state='disabled')
        self.text_widget.update_idletasks()

# ==============================================================================
# SCRIPT 1: Zimageä»ai-toolkitè½¬æ¢è¡¥å…¨å±‚çº§.py (æ ¸å¿ƒé€»è¾‘)
# ==============================================================================
def convert_lora_for_comfyui(input_path, output_path, logger=None, progress_callback=None):
    """
    å°† ai-toolkit çš„ LoRA è½¬æ¢ä¸º ComfyUI å…¼å®¹æ ¼å¼ã€‚
    æ ¸å¿ƒåŠŸèƒ½æ˜¯ä» Zimage...py è„šæœ¬ä¸­æå–çš„ã€‚
    """
    if logger: logger.log("å¼€å§‹ LoRA æ ¼å¼è½¬æ¢...")
    if progress_callback: progress_callback(0, "æ­£åœ¨åŠ è½½ LoRA æ–‡ä»¶...")

    try:
        lora_dict = load_file(input_path)
    except Exception as e:
        raise ValueError(f"åŠ è½½ LoRA æ–‡ä»¶å¤±è´¥: {e}")

    total_keys = len(lora_dict)
    if logger: logger.log(f"LoRA åŠ è½½å®Œæˆï¼Œå…± {total_keys} ä¸ªé”®ã€‚")
    if progress_callback: progress_callback(5, f"åˆ†æé”®å€¼...")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢
    needs_conversion = any('.attention.to_q.' in k for k in lora_dict.keys())
    if not needs_conversion:
        if logger: logger.log("LoRA ä¼¼ä¹å·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œæ— éœ€è½¬æ¢ã€‚å°†ç›´æ¥å¤åˆ¶æ–‡ä»¶ã€‚")
        if progress_callback: progress_callback(100, "æ— éœ€è½¬æ¢ï¼Œè·³è¿‡æ­¤æ­¥éª¤ã€‚")
        save_file(lora_dict, output_path) # ç®€å•å¤åˆ¶
        return 0 # è¿”å›0è¡¨ç¤ºæ²¡æœ‰å±‚è¢«è½¬æ¢

    layer_groups = defaultdict(lambda: defaultdict(dict))
    output_dict = {}
    
    processed = 0
    for key, value in lora_dict.items():
        processed += 1
        if progress_callback and processed % 50 == 0:
            progress_callback(5 + 45 * processed // total_keys, f"è§£æé”®... ({processed}/{total_keys})")

        if '.attention.to_out.0.' in key:
            new_key = key.replace('.to_out.0.', '.out.')
            output_dict[new_key] = value
            if 'lora_A' in key:
                base = key.rsplit('.lora_A', 1)[0]
                alpha_key = f"{base}.alpha"
                if alpha_key in lora_dict:
                    new_alpha = alpha_key.replace('.to_out.0.', '.out.')
                    output_dict[new_alpha] = lora_dict[alpha_key]
            continue

        if '.attention.to_' in key and '.alpha' in key:
            continue

        if '.attention.to_' in key and any(x in key for x in ('.to_q.', '.to_k.', '.to_v.')):
            parts = key.split('.')
            layer_idx, attn_type, lora_type = None, None, None
            for i, p in enumerate(parts):
                if p == 'layers' and i + 1 < len(parts): layer_idx = parts[i + 1]
                elif p in ('to_q', 'to_k', 'to_v'): attn_type = p[3:]
                elif p in ('lora_A', 'lora_B'): lora_type = p
            
            if layer_idx and attn_type and lora_type:
                base_parts = [p for p in parts if p not in ('to_q', 'to_k', 'to_v')]
                base_key = '.'.join(base_parts[:-2])
                layer_groups[base_key][attn_type][lora_type] = value
                continue

        output_dict[key] = value

    if logger: logger.log(f"æ‰¾åˆ° {len(layer_groups)} ä¸ªéœ€è¦åˆå¹¶çš„æ³¨æ„åŠ›å±‚ã€‚")
    if progress_callback: progress_callback(50, "å¼€å§‹åˆå¹¶ qkv å±‚...")

    converted_count = 0
    step = 40.0 / max(len(layer_groups), 1)
    current = 0
    for base_key, qkv_dict in layer_groups.items():
        current += 1
        if progress_callback:
            progress_callback(50 + step * current, f"åˆå¹¶å±‚ {current}/{len(layer_groups)}")

        if not all(x in qkv_dict for x in ('q', 'k', 'v')): continue
        qB, kB, vB = qkv_dict['q'].get('lora_B'), qkv_dict['k'].get('lora_B'), qkv_dict['v'].get('lora_B')
        qA, kA, vA = qkv_dict['q'].get('lora_A'), qkv_dict['k'].get('lora_A'), qkv_dict['v'].get('lora_A')

        if None in (qB, kB, vB, qA, kA, vA): continue

        try:
            hidden_dim, rank = qB.shape
            qkv_B = torch.zeros(3 * hidden_dim, 3 * rank, dtype=qB.dtype)
            qkv_B[:hidden_dim, :rank] = qB
            qkv_B[hidden_dim:2*hidden_dim, rank:2*rank] = kB
            qkv_B[2*hidden_dim:, 2*rank:] = vB
            qkv_A = torch.cat([qA, kA, vA], dim=0)

            output_dict[f"{base_key}.qkv.lora_B.weight"] = qkv_B
            output_dict[f"{base_key}.qkv.lora_A.weight"] = qkv_A
            converted_count += 1

            alpha_key_q = f"{base_key}.to_q.alpha"
            orig_alpha = lora_dict.get(alpha_key_q) or lora_dict.get(f"{base_key}.to_q.lora_A.alpha")
            if orig_alpha is not None:
                output_dict[f"{base_key}.qkv.alpha"] = orig_alpha * 3.0
        except Exception as e:
            if logger: logger.log(f"åˆå¹¶å±‚ {base_key} å¤±è´¥: {e}", "ERROR")

    if progress_callback: progress_callback(95, "æ­£åœ¨ä¿å­˜ä¿®å¤åçš„ LoRA...")
    
    metadata = {}
    try:
        with safe_open(input_path, framework="pt", device="cpu") as f:
            metadata = f.metadata() or {}
    except Exception: pass
    metadata['converted_by'] = 'All-in-One Workflow GUI'
    
    save_file(output_dict, output_path, metadata=metadata)
    if logger: logger.log(f"LoRA ä¿®å¤å®Œæˆï¼Œå…±è½¬æ¢ {converted_count} ä¸ªæ³¨æ„åŠ›å±‚ã€‚")
    if progress_callback: progress_callback(100, "LoRA ä¿®å¤å®Œæˆã€‚")
    return converted_count

# ==============================================================================
# SCRIPT 2: nextdit_lora_merger_AB.py (æ ¸å¿ƒé€»è¾‘)
# ==============================================================================
def merge_lora_A_B_with_base_model(base_model_path, lora_path, output_path, lora_strength=1.0, logger=None, progress_callback=None):
    """
    èåˆ lora_A/B æ ¼å¼çš„ LoRA åˆ°åŸºç¡€æ¨¡å‹ã€‚
    æ ¸å¿ƒåŠŸèƒ½æ˜¯ä» nextdit_lora_merger_AB.py è„šæœ¬ä¸­æå–çš„ã€‚
    """
    if logger:
        logger.log("=== å¼€å§‹æ¨¡å‹ä¸ LoRA èåˆ ===")
        logger.log(f"åŸºç¡€æ¨¡å‹: {Path(base_model_path).name}")
        logger.log(f"LoRA: {Path(lora_path).name}")
        logger.log(f"æƒé‡: {lora_strength}")

    # åŠ è½½åŸºç¡€æ¨¡å‹
    if progress_callback: progress_callback(0, "æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")
    try:
        base_model = load_file(base_model_path)
        if logger: logger.log(f"åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ: {len(base_model.keys())} ä¸ªå¼ é‡ã€‚")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½åŸºç¡€æ¨¡å‹å¤±è´¥: {e}")

    # åŠ è½½LoRA
    if progress_callback: progress_callback(20, "æ­£åœ¨åŠ è½½ LoRA...")
    try:
        lora_model = load_file(lora_path)
        if logger: logger.log(f"LoRA åŠ è½½æˆåŠŸ: {len(lora_model.keys())} ä¸ªå¼ é‡ã€‚")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½ LoRA å¤±è´¥: {e}")

    # æå– lora_A/B å¯¹
    if progress_callback: progress_callback(40, "æ­£åœ¨æå– LoRA A/B å¯¹...")
    lora_pairs = {}
    prefix_to_remove = 'diffusion_model.' if any(k.startswith('diffusion_model.') for k in lora_model.keys()) else None
    
    for key in lora_model.keys():
        if '.lora_A.weight' in key:
            clean_key = key
            if prefix_to_remove and clean_key.startswith(prefix_to_remove):
                clean_key = clean_key[len(prefix_to_remove):]
            
            base_key = clean_key.replace('.lora_A.weight', '.weight')
            lora_b_key = key.replace('.lora_A.weight', '.lora_B.weight')
            
            if lora_b_key in lora_model:
                lora_pairs[base_key] = {'A': lora_model[key], 'B': lora_model[lora_b_key]}
    
    if logger: logger.log(f"æ‰¾åˆ° {len(lora_pairs)} ä¸ª LoRA A/B å¯¹ã€‚")
    if not lora_pairs:
        raise ValueError("åœ¨ LoRA æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'lora_A/lora_B' æ ¼å¼çš„æƒé‡ã€‚è¯·æ£€æŸ¥ LoRA æ–‡ä»¶ã€‚")

    # åº”ç”¨ LoRA
    if progress_callback: progress_callback(60, "æ­£åœ¨åº”ç”¨ LoRA æƒé‡...")
    merged_model = base_model.copy()
    applied_count, skipped_count = 0, 0
    
    total_pairs = len(lora_pairs)
    processed_pairs = 0
    for base_key, pair_info in lora_pairs.items():
        processed_pairs += 1
        if progress_callback:
            progress_callback(60 + 35 * processed_pairs // total_pairs, f"åº”ç”¨å±‚ {processed_pairs}/{total_pairs}")

        if base_key in merged_model:
            base_weight = merged_model[base_key]
            lora_A, lora_B = pair_info['A'], pair_info['B']
            
            try:
                if len(lora_A.shape) == 2 and len(lora_B.shape) == 2:
                    lora_delta = torch.mm(lora_B, lora_A) * lora_strength
                elif len(lora_A.shape) == 4 and len(lora_B.shape) == 4:
                    lora_delta = torch.nn.functional.conv2d(lora_A.permute(1, 0, 2, 3), lora_B).permute(1, 0, 2, 3) * lora_strength
                else:
                    if logger: logger.log(f"è·³è¿‡ä¸æ”¯æŒçš„ LoRA å½¢çŠ¶: A: {lora_A.shape}, B: {lora_B.shape}", "WARN")
                    continue
                
                if base_weight.shape != lora_delta.shape:
                    if logger: logger.log(f"è·³è¿‡å½¢çŠ¶ä¸åŒ¹é…çš„å±‚: base: {base_weight.shape}, delta: {lora_delta.shape}", "WARN")
                    continue

                merged_model[base_key] = base_weight + lora_delta.to(base_weight.dtype)
                applied_count += 1
            except Exception as e:
                if logger: logger.log(f"åº”ç”¨ LoRA åˆ° {base_key} æ—¶å‡ºé”™: {e}", "ERROR")
                skipped_count += 1
        else:
            skipped_count += 1

    if logger:
        logger.log(f"æˆåŠŸåº”ç”¨ LoRA åˆ° {applied_count} ä¸ªå±‚ã€‚")
        if skipped_count > 0:
            logger.log(f"è·³è¿‡ {skipped_count} ä¸ªå±‚ (åœ¨åŸºç¡€æ¨¡å‹ä¸­æœªæ‰¾åˆ°æˆ–å‡ºé”™)ã€‚", "WARN")
    
    if applied_count == 0:
        raise RuntimeError("é”™è¯¯: æ²¡æœ‰ä»»ä½• LoRA å±‚è¢«åº”ç”¨ï¼è¿™å¾ˆå¯èƒ½æ˜¯å› ä¸ºåŸºç¡€æ¨¡å‹å’Œ LoRA ä¹‹é—´çš„é”®åä¸åŒ¹é…ã€‚")

    # ä¿å­˜èåˆåçš„æ¨¡å‹
    if progress_callback: progress_callback(95, "æ­£åœ¨ä¿å­˜èåˆåçš„æ¨¡å‹...")
    try:
        save_file(merged_model, output_path)
        if logger: logger.log(f"èåˆåçš„æ¨¡å‹å·²ä¿å­˜ã€‚")
    except Exception as e:
        raise RuntimeError(f"ä¿å­˜èåˆæ¨¡å‹å¤±è´¥: {e}")
    
    if progress_callback: progress_callback(100, "æ¨¡å‹èåˆå®Œæˆã€‚")
    return merged_model

# ==============================================================================
# SCRIPT 3: convert_quantize.py (æ ¸å¿ƒé€»è¾‘)
# ==============================================================================
# --- ä» convert_quantize.py è„šæœ¬ä¸­æå–çš„è¾…åŠ©ç±»å’Œå‡½æ•° ---
QUANTIZATION_THRESHOLD = 1024
REARRANGE_THRESHOLD = 512
MAX_TENSOR_NAME_LENGTH = 127
MAX_TENSOR_DIMS = 4

class ModelTemplate:
    arch, shape_fix = "invalid", False
    keys_detect, keys_banned, keys_hiprec, keys_ignore = [], [], [], []
    def handle_nd_tensor(self, key, data): raise NotImplementedError(f"Tensor >4D: {key} @ {data.shape}")

class ModelFlux(ModelTemplate):
    arch = "flux"; keys_detect = [("transformer_blocks.0.attn.norm_added_k.weight",), ("double_blocks.0.img_attn.proj.weight",)]; keys_banned = ["transformer_blocks.0.attn.norm_added_k.weight",]
class ModelSD3(ModelTemplate):
    arch = "sd3"; keys_detect = [("transformer_blocks.0.attn.add_q_proj.weight",), ("joint_blocks.0.x_block.attn.qkv.weight",)]; keys_banned = ["transformer_blocks.0.attn.add_q_proj.weight",]
class ModelAura(ModelTemplate):
    arch = "aura"; keys_detect = [("double_layers.3.modX.1.weight",), ("joint_transformer_blocks.3.ff_context.out_projection.weight",)]; keys_banned = ["joint_transformer_blocks.3.ff_context.out_projection.weight",]
class ModelHiDream(ModelTemplate):
    arch = "hidream"; keys_detect = [("caption_projection.0.linear.weight", "double_stream_blocks.0.block.ff_i.shared_experts.w3.weight")]; keys_hiprec = [".ff_i.gate.weight", "img_emb.emb_pos"]
class CosmosPredict2(ModelTemplate):
    arch = "cosmos"; keys_detect = [("blocks.0.mlp.layer1.weight", "blocks.0.adaln_modulation_cross_attn.1.weight")]; keys_hiprec = ["pos_embedder"]; keys_ignore = ["_extra_state", "accum_"]
class ModelHyVid(ModelTemplate):
    arch = "hyvid"; keys_detect = [("double_blocks.0.img_attn_proj.weight", "txt_in.individual_token_refiner.blocks.1.self_attn_qkv.weight")]
    def handle_nd_tensor(self, key, data): raise RuntimeError(f"5D tensor fix file already exists!")
class ModelWan(ModelHyVid):
    arch = "wan"; keys_detect = [("blocks.0.self_attn.norm_q.weight", "text_embedding.2.weight", "head.modulation")]; keys_hiprec = [".modulation"]
class ModelLTXV(ModelTemplate):
    arch = "ltxv"; keys_detect = [("adaln_single.emb.timestep_embedder.linear_2.weight", "transformer_blocks.27.scale_shift_table", "caption_projection.linear_2.weight")]; keys_hiprec = ["scale_shift_table"]
class ModelSDXL(ModelTemplate):
    arch = "sdxl"; shape_fix = True; keys_detect = [("down_blocks.0.downsamplers.0.conv.weight", "add_embedding.linear_1.weight",), ("input_blocks.3.0.op.weight", "input_blocks.6.0.op.weight", "output_blocks.2.2.conv.weight", "output_blocks.5.2.conv.weight"), ("label_emb.0.0.weight",)]
class ModelSD1(ModelTemplate):
    arch = "sd1"; shape_fix = True; keys_detect = [("down_blocks.0.downsamplers.0.conv.weight",), ("input_blocks.3.0.op.weight", "input_blocks.6.0.op.weight", "input_blocks.9.0.op.weight", "output_blocks.2.1.conv.weight", "output_blocks.5.2.conv.weight", "output_blocks.8.2.conv.weight")]
class ModelLumina2(ModelTemplate):
    arch = "lumina2"; keys_detect = [("cap_embedder.1.weight", "context_refiner.0.attention.qkv.weight")]; keys_hiprec = ["pad_token"]

arch_list = [ModelFlux, ModelSD3, ModelAura, ModelHiDream, CosmosPredict2, ModelLTXV, ModelHyVid, ModelWan, ModelSDXL, ModelSD1, ModelLumina2]

def detect_arch(state_dict):
    for arch_cls in arch_list:
        matched, invalid = False, False
        for match_list in arch_cls.keys_detect:
            if all(key in state_dict for key in match_list):
                matched = True
                invalid = any(key in state_dict for key in arch_cls.keys_banned)
                break
        if matched and not invalid: return arch_cls()
    raise RuntimeError("æ— æ³•è¯†åˆ«çš„æ¨¡å‹æ¶æ„ï¼")

def get_quant_type(name): return getattr(GGMLQuantizationType, name, None)
def get_file_type(name): return getattr(LlamaFileType, name, None)

def test_quantization(qtype):
    try:
        test_data = np.random.randn(256, 256).astype(np.float32)
        gguf.quants.quantize(test_data, qtype)
        return True
    except (NotImplementedError, Exception):
        return False

QUANT_MAP_UNTESTED = {
    key: (get_quant_type(qtype_name), get_file_type(ftype_name) if ftype_name else None, desc)
    for key, qtype_name, ftype_name, desc in [
        ('f32', 'F32', None, "32-bit float (æ— æŸ)"), ('f16', 'F16', 'MOSTLY_F16', "16-bit float"), ('bf16', 'BF16', 'MOSTLY_BF16', "bfloat16"),
        ('q2_k', 'Q2_K', 'MOSTLY_Q2_K', "2-bit K-quant"), ('q3_k', 'Q3_K', 'MOSTLY_Q3_K_M', "3-bit K-quant"),
        ('q4_0', 'Q4_0', 'MOSTLY_Q4_0', "4-bit (legacy)"), ('q4_1', 'Q4_1', 'MOSTLY_Q4_1', "4-bit (legacy, æ›´å¥½)"),
        ('q4_k_m', 'Q4_K_M', 'MOSTLY_Q4_K_M', "4-bit K-quant (æ¨è)"), ('q5_0', 'Q5_0', 'MOSTLY_Q5_0', "5-bit (legacy)"),
        ('q5_1', 'Q5_1', 'MOSTLY_Q5_1', "5-bit (legacy, æ›´å¥½)"), ('q5_k_m', 'Q5_K_M', 'MOSTLY_Q5_K_M', "5-bit K-quant"),
        ('q6_k', 'Q6_K', 'MOSTLY_Q6_K', "6-bit K-quant"), ('q8_0', 'Q8_0', 'MOSTLY_Q8_0', "8-bit (é‡åŒ–ä¸­æœ€ä½³)")
    ] if get_quant_type(qtype_name) is not None
}
QUANT_MAP_TESTED, QUANT_BROKEN = None, None

def get_tested_quant_map(logger=None):
    global QUANT_MAP_TESTED, QUANT_BROKEN
    if QUANT_MAP_TESTED is None:
        if logger: logger.log("æ­£åœ¨æµ‹è¯•å½“å‰ç¯å¢ƒæ”¯æŒçš„é‡åŒ–ç±»å‹...")
        working, broken = {}, []
        for key, (qtype, ftype, desc) in QUANT_MAP_UNTESTED.items():
            if test_quantization(qtype): working[key] = (qtype, ftype, desc)
            else: broken.append(key)
        QUANT_MAP_TESTED, QUANT_BROKEN = working, broken
        if logger:
            logger.log(f"æµ‹è¯•å®Œæˆã€‚å¯ç”¨ç±»å‹: {len(working)}ä¸ª, ä¸å¯ç”¨: {len(broken)}ä¸ªã€‚")
            if broken: logger.log(f"ä¸å¯ç”¨ç±»å‹: {', '.join(broken)}", "WARN")
    return QUANT_MAP_TESTED

def convert_to_gguf(state_dict, dst_path, quant_type='bf16', logger=None, progress_callback=None):
    """
    å°† state_dict è½¬æ¢ä¸º GGUF å¹¶é‡åŒ–ã€‚
    æ ¸å¿ƒåŠŸèƒ½æ˜¯ä» convert_quantize.py è„šæœ¬ä¸­æå–çš„ã€‚
    """
    if logger:
        logger.log("=== å¼€å§‹ GGUF è½¬æ¢ä¸é‡åŒ– ===")
        logger.log(f"é‡åŒ–ç±»å‹: {quant_type.upper()}")

    # è¯†åˆ«æ¶æ„
    if progress_callback: progress_callback(0, "è¯†åˆ«æ¨¡å‹æ¶æ„...")
    try:
        model_arch = detect_arch(state_dict)
        if logger: logger.log(f"è¯†åˆ«åˆ°æ¨¡å‹æ¶æ„: {model_arch.arch}")
    except Exception as e:
        raise RuntimeError(f"è¯†åˆ«æ¨¡å‹æ¶æ„å¤±è´¥: {e}")

    # è·å–é‡åŒ–é…ç½®
    working_quants = get_tested_quant_map(logger)
    quant_key = quant_type.lower()
    if quant_key not in working_quants:
        raise ValueError(f"ä¸æ”¯æŒçš„é‡åŒ–ç±»å‹: {quant_type}ã€‚è¯·å…ˆæµ‹è¯•å¯ç”¨ç±»å‹ã€‚")
    
    target_qtype, ftype_gguf, desc = working_quants[quant_key]
    if logger: logger.log(f"ä½¿ç”¨ç±»å‹: {desc}")

    fallback_qtype = working_quants.get('f16', working_quants.get('bf16', working_quants.get('f32')))[0]
    if logger: logger.log(f"å¤‡ç”¨é‡åŒ–ç±»å‹: {fallback_qtype.name}")

    # åˆ›å»º GGUF writer
    if progress_callback: progress_callback(10, "åˆ›å»º GGUF æ–‡ä»¶å¤´...")
    writer = GGUFWriter(path=None, arch=model_arch.arch)
    writer.add_quantization_version(gguf.GGML_QUANT_VERSION)
    if ftype_gguf is not None: writer.add_file_type(ftype_gguf)

    # å¤„ç†å¼ é‡
    if progress_callback: progress_callback(20, "å¼€å§‹å¤„ç†å¼ é‡...")
    stats = {'quantized': 0, 'kept_f32': 0, 'fallback': 0, 'total': 0}
    total_tensors = len(state_dict)
    processed_tensors = 0

    for key, data in state_dict.items():
        processed_tensors += 1
        if progress_callback:
            progress_callback(20 + 70 * processed_tensors // total_tensors, f"å¤„ç†å¼ é‡ {processed_tensors}/{total_tensors}")

        stats['total'] += 1
        if any(x in key for x in model_arch.keys_ignore): continue

        if data.dtype == torch.bfloat16: data = data.to(torch.float32).numpy()
        else: data = data.numpy()

        n_dims = len(data.shape)
        if n_dims > MAX_TENSOR_DIMS:
            if logger: logger.log(f"è·³è¿‡ >4D å¼ é‡: {key} {data.shape}", "WARN")
            continue

        n_params = np.prod(data.shape)
        data_qtype = target_qtype
        if n_dims == 1 or n_params <= QUANTIZATION_THRESHOLD or any(x in key for x in model_arch.keys_hiprec):
            data_qtype = GGMLQuantizationType.F32
            stats['kept_f32'] += 1
        else:
            stats['quantized'] += 1

        try:
            quantized_data = gguf.quants.quantize(data, data_qtype)
        except Exception:
            if logger: logger.log(f"é‡åŒ– {key} å¤±è´¥, å°è¯•å¤‡ç”¨ç±»å‹ {fallback_qtype.name}", "WARN")
            data_qtype = fallback_qtype
            quantized_data = gguf.quants.quantize(data, data_qtype)
            stats['fallback'] += 1
        
        writer.add_tensor(key, quantized_data, raw_dtype=data_qtype)

    # å†™å…¥æ–‡ä»¶
    if progress_callback: progress_callback(95, "æ­£åœ¨å†™å…¥ GGUF æ–‡ä»¶...")
    writer.write_header_to_file(path=dst_path)
    writer.write_kv_data_to_file()
    writer.write_tensors_to_file()
    writer.close()

    if logger:
        logger.log("GGUF æ–‡ä»¶å†™å…¥å®Œæˆã€‚")
        logger.log(f"ç»Ÿè®¡: æ€»å¼ é‡={stats['total']}, é‡åŒ–={stats['quantized']}, ä¿æŒF32={stats['kept_f32']}, ä½¿ç”¨å¤‡ç”¨={stats['fallback']}")
    if progress_callback: progress_callback(100, "GGUF è½¬æ¢å®Œæˆã€‚")

# ==============================================================================
# GUI ä¸»åº”ç”¨
# ==============================================================================
class WorkflowGUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("æ¨¡å‹èåˆä¸GGUFé‡åŒ–å·¥ä½œæµ")
        self.geometry("800x650")
        self.resizable(True, True)

        style = ttk.Style(self)
        style.theme_use('clam')

        main_frame = ttk.Frame(self, padding=20)
        main_frame.pack(fill=tk.BOTH, expand=True)
        main_frame.columnconfigure(1, weight=1)

        # --- è¾“å…¥/è¾“å‡ºè®¾ç½® ---
        io_frame = ttk.LabelFrame(main_frame, text="æ–‡ä»¶è·¯å¾„", padding=10)
        io_frame.grid(row=0, column=0, columnspan=3, sticky="ew", pady=5)
        io_frame.columnconfigure(1, weight=1)

        self.base_model_path = tk.StringVar()
        self.lora_path = tk.StringVar()
        self.output_path = tk.StringVar()

        ttk.Label(io_frame, text="åŸºç¡€æ¨¡å‹:").grid(row=0, column=0, sticky="w", padx=5, pady=5)
        ttk.Entry(io_frame, textvariable=self.base_model_path).grid(row=0, column=1, sticky="ew", padx=5)
        ttk.Button(io_frame, text="æµè§ˆ...", command=lambda: self.browse_file(self.base_model_path, "é€‰æ‹©åŸºç¡€æ¨¡å‹", [("Safetensors", "*.safetensors")])).grid(row=0, column=2, padx=5)

        ttk.Label(io_frame, text="LoRAæ¨¡å‹:").grid(row=1, column=0, sticky="w", padx=5, pady=5)
        ttk.Entry(io_frame, textvariable=self.lora_path).grid(row=1, column=1, sticky="ew", padx=5)
        ttk.Button(io_frame, text="æµè§ˆ...", command=lambda: self.browse_file(self.lora_path, "é€‰æ‹©LoRAæ¨¡å‹", [("Safetensors", "*.safetensors")])).grid(row=1, column=2, padx=5)

        ttk.Label(io_frame, text="è¾“å‡ºæ–‡ä»¶:").grid(row=2, column=0, sticky="w", padx=5, pady=5)
        ttk.Entry(io_frame, textvariable=self.output_path).grid(row=2, column=1, sticky="ew", padx=5)
        ttk.Button(io_frame, text="å¦å­˜ä¸º...", command=self.browse_save_file).grid(row=2, column=2, padx=5)

        # --- å‚æ•°è®¾ç½® ---
        params_frame = ttk.LabelFrame(main_frame, text="å‚æ•°è®¾ç½®", padding=10)
        params_frame.grid(row=1, column=0, columnspan=3, sticky="ew", pady=10)
        params_frame.columnconfigure(1, weight=1)

        self.lora_weight = tk.DoubleVar(value=1.0)
        ttk.Label(params_frame, text="LoRA æƒé‡:").grid(row=0, column=0, sticky="w", padx=5, pady=5)
        ttk.Spinbox(params_frame, from_=0.0, to=5.0, increment=0.1, textvariable=self.lora_weight, width=10).grid(row=0, column=1, sticky="w", padx=5)

        ttk.Label(params_frame, text="é‡åŒ–ç±»å‹:").grid(row=1, column=0, sticky="w", padx=5, pady=5)
        self.quant_type = tk.StringVar()
        self.quant_combo = ttk.Combobox(params_frame, textvariable=self.quant_type, state="readonly")
        self.quant_combo.grid(row=1, column=1, sticky="ew", padx=5)
        self.test_quant_btn = ttk.Button(params_frame, text="æµ‹è¯•å¹¶åˆ—å‡ºå¯ç”¨é‡åŒ–ç±»å‹", command=self.test_and_list_quants)
        self.test_quant_btn.grid(row=1, column=2, padx=5)
        self.quant_combo['values'] = ['bf16', 'f16', 'f32']
        self.quant_type.set('q4_k_m') # é»˜è®¤æ¨èå€¼

        # --- æ§åˆ¶ä¸è¿›åº¦ ---
        self.progress_label = ttk.Label(main_frame, text="å°±ç»ª")
        self.progress_label.grid(row=2, column=0, columnspan=3, sticky="ew", pady=(10, 0))
        
        self.progress = ttk.Progressbar(main_frame, mode='determinate')
        self.progress.grid(row=3, column=0, columnspan=3, sticky="ew", pady=5)

        self.start_btn = ttk.Button(main_frame, text="å¼€å§‹å·¥ä½œæµ", command=self.start_workflow)
        self.start_btn.grid(row=4, column=0, columnspan=3, pady=10)

        # --- æ—¥å¿—åŒºåŸŸ ---
        log_frame = ttk.LabelFrame(main_frame, text="å®æ—¶æ—¥å¿—", padding=10)
        log_frame.grid(row=5, column=0, columnspan=3, sticky="nsew", pady=10)
        log_frame.rowconfigure(0, weight=1)
        log_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(5, weight=1)

        self.log_text = tk.Text(log_frame, height=10, state='disabled', font=("Consolas", 9), wrap="word")
        scrollbar = ttk.Scrollbar(log_frame, orient="vertical", command=self.log_text.yview)
        self.log_text.configure(yscrollcommand=scrollbar.set)
        self.log_text.grid(row=0, column=0, sticky="nsew")
        scrollbar.grid(row=0, column=1, sticky="ns")
        
        self.logger = GuiLogger(self.log_text)

    def browse_file(self, var, title, filetypes):
        path = filedialog.askopenfilename(title=title, filetypes=filetypes)
        if path:
            var.set(path)
            if var == self.base_model_path:
                self.auto_fill_output()

    def browse_save_file(self):
        path = filedialog.asksaveasfilename(title="ä¿å­˜ GGUF æ–‡ä»¶", defaultextension=".gguf", filetypes=[("GGUF", "*.gguf")])
        if path:
            self.output_path.set(path)

    def auto_fill_output(self):
        base_path = self.base_model_path.get()
        if base_path:
            dir_name = os.path.dirname(base_path)
            base_name = os.path.splitext(os.path.basename(base_path))[0]
            quant = self.quant_type.get().replace('_', '')
            suggested_name = f"{base_name}_merged_{quant}.gguf"
            self.output_path.set(os.path.join(dir_name, suggested_name))

    def update_progress(self, step_name, value, status):
        # æ€»è¿›åº¦æ¡åˆ†ä¸º3ä¸ªé˜¶æ®µï¼šLoRAä¿®å¤(10%) -> æ¨¡å‹èåˆ(40%) -> GGUFè½¬æ¢(50%)
        total_progress = 0
        if step_name == "LORA_FIX":
            total_progress = value * 0.1
        elif step_name == "MERGE":
            total_progress = 10 + (value * 0.4)
        elif step_name == "QUANTIZE":
            total_progress = 50 + (value * 0.5)
        
        self.progress['value'] = total_progress
        self.progress_label.config(text=f"æ­¥éª¤: {step_name} - {status}")
        self.update_idletasks()

    def test_and_list_quants(self):
        self.test_quant_btn.config(state="disabled")
        self.logger.log("å¼€å§‹æµ‹è¯•å¯ç”¨çš„é‡åŒ–ç±»å‹...")
        
        def run_test():
            try:
                working_quants = get_tested_quant_map(self.logger)
                quant_keys = sorted(working_quants.keys())
                self.quant_combo['values'] = quant_keys
                if 'q4_k_m' in quant_keys:
                    self.quant_type.set('q4_k_m')
                elif quant_keys:
                    self.quant_type.set(quant_keys[0])
                self.logger.log("å¯ç”¨é‡åŒ–ç±»å‹åˆ—è¡¨å·²æ›´æ–°ã€‚")
                messagebox.showinfo("æµ‹è¯•å®Œæˆ", f"æ‰¾åˆ° {len(quant_keys)} ä¸ªå¯ç”¨çš„é‡åŒ–ç±»å‹ï¼Œå·²æ›´æ–°ä¸‹æ‹‰åˆ—è¡¨ã€‚")
            except Exception as e:
                self.logger.log(f"æµ‹è¯•é‡åŒ–ç±»å‹æ—¶å‡ºé”™: {e}", "ERROR")
                messagebox.showerror("é”™è¯¯", f"æµ‹è¯•å¤±è´¥: {e}")
            finally:
                self.test_quant_btn.config(state="normal")

        threading.Thread(target=run_test, daemon=True).start()

    def start_workflow(self):
        base_model = self.base_model_path.get().strip()
        lora = self.lora_path.get().strip()
        output = self.output_path.get().strip()
        weight = self.lora_weight.get()
        quant = self.quant_type.get().strip()

        if not all([base_model, lora, output, quant]):
            messagebox.showerror("é”™è¯¯", "æ‰€æœ‰è·¯å¾„å’Œå‚æ•°éƒ½å¿…é¡»å¡«å†™ï¼")
            return

        if not os.path.isfile(base_model) or not os.path.isfile(lora):
            messagebox.showerror("é”™è¯¯", "åŸºç¡€æ¨¡å‹æˆ–LoRAæ–‡ä»¶ä¸å­˜åœ¨ï¼")
            return

        self.start_btn.config(state="disabled")
        self.log_text.config(state='normal')
        self.log_text.delete(1.0, tk.END)
        self.log_text.config(state='disabled')

        def run_workflow():
            temp_dir = tempfile.mkdtemp()
            fixed_lora_path = os.path.join(temp_dir, "fixed_lora.safetensors")
            merged_model_path = os.path.join(temp_dir, "merged_model.safetensors")
            
            try:
                # --- æ­¥éª¤ 1: ä¿®å¤ LoRA ---
                self.logger.log("--- æ­¥éª¤ 1/3: ä¿®å¤/è½¬æ¢ LoRA ---")
                convert_lora_for_comfyui(
                    lora, fixed_lora_path, self.logger,
                    lambda v, s: self.update_progress("LORA_FIX", v, s)
                )

                # --- æ­¥éª¤ 2: èåˆæ¨¡å‹ ---
                self.logger.log("\n--- æ­¥éª¤ 2/3: èåˆåŸºç¡€æ¨¡å‹ä¸ LoRA ---")
                merge_lora_A_B_with_base_model(
                    base_model, fixed_lora_path, merged_model_path, weight, self.logger,
                    lambda v, s: self.update_progress("MERGE", v, s)
                )

                # --- æ­¥éª¤ 3: GGUF è½¬æ¢å’Œé‡åŒ– ---
                self.logger.log("\n--- æ­¥éª¤ 3/3: è½¬æ¢ä¸º GGUF å¹¶é‡åŒ– ---")
                # åŠ è½½åˆšåˆšèåˆçš„æ¨¡å‹ state_dict
                merged_state_dict = load_file(merged_model_path)
                convert_to_gguf(
                    merged_state_dict, output, quant, self.logger,
                    lambda v, s: self.update_progress("QUANTIZE", v, s)
                )

                self.logger.log(f"\nğŸ‰ğŸ‰ğŸ‰ å·¥ä½œæµå…¨éƒ¨å®Œæˆï¼ğŸ‰ğŸ‰ğŸ‰")
                self.logger.log(f"æœ€ç»ˆæ–‡ä»¶å·²ä¿å­˜åˆ°: {output}")
                messagebox.showinfo("æˆåŠŸ", f"å·¥ä½œæµæ‰§è¡Œå®Œæ¯•ï¼\næœ€ç»ˆæ–‡ä»¶ä¿å­˜åœ¨:\n{output}")

            except Exception as e:
                error_msg = traceback.format_exc()
                self.logger.log(f"\nâŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥ï¼\n{error_msg}", "ERROR")
                messagebox.showerror("æ‰§è¡Œå¤±è´¥", f"å·¥ä½œæµä¸­å‘ç”Ÿé”™è¯¯ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—è·å–è¯¦æƒ…ã€‚\n\né”™è¯¯: {e}")
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    if os.path.exists(temp_dir):
                        import shutil
                        shutil.rmtree(temp_dir)
                        self.logger.log("ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")
                except Exception as e:
                    self.logger.log(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}", "WARN")
                
                self.start_btn.config(state="normal")
                self.progress['value'] = 0
                self.progress_label.config(text="å°±ç»ª")

        threading.Thread(target=run_workflow, daemon=True).start()


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–æ˜¯å¦æ»¡è¶³
    try:
        import torch, safetensors, gguf, numpy, tqdm
    except ImportError as e:
        # å¦‚æœåœ¨ä¸»ç¨‹åºä¸­æ•è·åˆ°ï¼Œè¯´æ˜æ˜¯é¦–æ¬¡è¿è¡Œï¼Œå¼¹å‡ºæç¤ºæ¡†
        root = tk.Tk()
        root.withdraw() # éšè—ä¸»çª—å£
        messagebox.showerror("ä¾èµ–ç¼ºå¤±", f"å¯åŠ¨å¤±è´¥ï¼Œç¼ºå°‘å¿…è¦çš„åº“: {e}\n\nè¯·åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“:\npip install torch safetensors gguf numpy tqdm")
        sys.exit(1)

    app = WorkflowGUI()
    app.mainloop()

