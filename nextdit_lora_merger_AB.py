#!/usr/bin/env python3
"""
NextDiT LoRA Merger (ÊîØÊåÅ lora_A/lora_B Ê†ºÂºè)
"""

import torch
import argparse
import os
import sys
from pathlib import Path

try:
    from safetensors.torch import load_file, save_file
except ImportError:
    print("Error: safetensors not installed. Please run: pip install safetensors")
    sys.exit(1)

def validate_file_exists(file_path, file_type):
    """È™åËØÅÊñá‰ª∂ÊòØÂê¶Â≠òÂú®"""
    if not os.path.exists(file_path):
        print(f"Error: {file_type} file not found: {file_path}")
        sys.exit(1)
    return True

def analyze_lora_format(lora_dict):
    """ÂàÜÊûêLoRAÁöÑÊ†ºÂºè"""
    formats = {
        'lora_A_B': 0,  # lora_A.weight + lora_B.weight
        'lora_up_down': 0,  # lora_up.weight + lora_down.weight
        'other': 0
    }
    
    prefixes = set()
    
    for key in lora_dict.keys():
        # Êî∂ÈõÜÂâçÁºÄ
        if 'diffusion_model.' in key:
            prefixes.add('diffusion_model.')
        
        # ÂàÜÊûêÊ†ºÂºè
        if '.lora_A.weight' in key:
            formats['lora_A_B'] += 1
        elif '.lora_up.weight' in key:
            formats['lora_up_down'] += 1
        else:
            formats['other'] += 1
    
    main_format = max(formats, key=formats.get)
    return main_format, formats, prefixes

def extract_lora_A_B_pairs(lora_dict, prefix_to_remove='diffusion_model.'):
    """ÊèêÂèñlora_A/lora_BÊùÉÈáçÂØπÂπ∂ÁßªÈô§ÂâçÁºÄ"""
    lora_pairs = {}
    
    for key in lora_dict.keys():
        if '.lora_A.weight' in key:
            # ÁßªÈô§ÂâçÁºÄÂπ∂Ëé∑ÂèñÂü∫Á°ÄÂ±ÇÂêçÁß∞
            clean_key = key
            if prefix_to_remove and clean_key.startswith(prefix_to_remove):
                clean_key = clean_key[len(prefix_to_remove):]
            
            base_key = clean_key.replace('.lora_A.weight', '.weight')
            lora_b_key = key.replace('.lora_A.weight', '.lora_B.weight')
            
            if lora_b_key in lora_dict:
                lora_pairs[base_key] = {
                    'A': lora_dict[key],  # ÂØπÂ∫îdown
                    'B': lora_dict[lora_b_key],  # ÂØπÂ∫îup
                    'original_A_key': key,
                    'original_B_key': lora_b_key
                }
    
    return lora_pairs

def apply_lora_A_B_to_weight(base_weight, lora_A, lora_B, strength):
    """Â∞ÜLoRA_A/BÂ¢ûÈáèÂ∫îÁî®Âà∞Âü∫Á°ÄÊùÉÈáç"""
    try:
        # LoRAÂ¢ûÈáè = lora_B @ lora_A (Ê≥®ÊÑèÈ°∫Â∫èÔºÅ)
        if len(lora_A.shape) == 2 and len(lora_B.shape) == 2:
            lora_delta = torch.mm(lora_B, lora_A) * strength
        elif len(lora_A.shape) == 4 and len(lora_B.shape) == 4:
            lora_delta = torch.nn.functional.conv2d(
                lora_A.permute(1, 0, 2, 3), 
                lora_B
            ).permute(1, 0, 2, 3) * strength
        else:
            print(f"Warning: Unsupported LoRA shape - A: {lora_A.shape}, B: {lora_B.shape}")
            return base_weight
        
        # È™åËØÅÁª¥Â∫¶ÂåπÈÖç
        if base_weight.shape != lora_delta.shape:
            print(f"Warning: Shape mismatch - base: {base_weight.shape}, delta: {lora_delta.shape}")
            return base_weight
        
        # Â∫îÁî®Â¢ûÈáè
        return base_weight + lora_delta
    
    except Exception as e:
        print(f"Error applying LoRA: {e}")
        return base_weight

def merge_lora_A_B_with_base_model(base_model_path, lora_path, output_path, lora_strength=1.0, verbose=False):
    """ËûçÂêàlora_A/BÊ†ºÂºèÁöÑLoRAÂà∞Âü∫Á°ÄÊ®°Âûã"""
    print("=== NextDiT LoRA Merger (lora_A/BÊ†ºÂºè) ===")
    print(f"Base Model: {base_model_path}")
    print(f"LoRA: {lora_path}")
    print(f"Output: {output_path}")
    print(f"LoRA Strength: {lora_strength}")
    
    # È™åËØÅËæìÂÖ•Êñá‰ª∂
    validate_file_exists(base_model_path, "Base model")
    validate_file_exists(lora_path, "LoRA")
    
    # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
    
    # Âä†ËΩΩÂü∫Á°ÄÊ®°Âûã
    print("\nüì• Loading base model...")
    try:
        base_model = load_file(base_model_path)
        print(f"‚úì Base model loaded: {len(base_model.keys())} parameters")
    except Exception as e:
        print(f"Error loading base model: {e}")
        sys.exit(1)
    
    # Âä†ËΩΩLoRA
    print("\nüì• Loading LoRA...")
    try:
        lora_model = load_file(lora_path)
        print(f"‚úì LoRA loaded: {len(lora_model.keys())} parameters")
    except Exception as e:
        print(f"Error loading LoRA: {e}")
        sys.exit(1)
    
    # ÂàÜÊûêLoRAÊ†ºÂºè
    print("\nüîç Analyzing LoRA format...")
    lora_format, format_counts, prefixes = analyze_lora_format(lora_model)
    print(f"‚úì LoRA format detected: {lora_format}")
    print(f"  Format counts: {format_counts}")
    print(f"  Prefixes found: {prefixes}")
    
    if lora_format != 'lora_A_B':
        print(f"‚ùå Error: This tool is designed for lora_A/B format, but found: {lora_format}")
        sys.exit(1)
    
    # Á°ÆÂÆöË¶ÅÁßªÈô§ÁöÑÂâçÁºÄ
    prefix_to_remove = 'diffusion_model.' if 'diffusion_model.' in prefixes else None
    if prefix_to_remove:
        print(f"‚úì Will remove prefix: '{prefix_to_remove}'")
    
    # ÊèêÂèñLoRAÂØπ
    print("\nüîç Extracting LoRA A/B pairs...")
    lora_pairs = extract_lora_A_B_pairs(lora_model, prefix_to_remove)
    print(f"‚úì Found {len(lora_pairs)} LoRA A/B pairs")
    
    if verbose:
        print("üìã LoRA pairs preview (first 10):")
        for i, (base_key, pair_info) in enumerate(list(lora_pairs.items())[:10]):
            A_shape = pair_info['A'].shape
            B_shape = pair_info['B'].shape
            print(f"  {i+1}. {base_key}")
            print(f"     A: {A_shape} (from {pair_info['original_A_key']})")
            print(f"     B: {B_shape} (from {pair_info['original_B_key']})")
    
    # ÂàõÂª∫ËûçÂêàÂêéÁöÑÊ®°Âûã
    print("\nüîÑ Merging models...")
    merged_model = base_model.copy()
    applied_count = 0
    skipped_count = 0
    
    # Â∫îÁî®LoRAÊùÉÈáç
    for base_key, pair_info in lora_pairs.items():
        if base_key in merged_model:
            original_weight = merged_model[base_key]
            merged_weight = apply_lora_A_B_to_weight(
                original_weight,
                pair_info['A'],
                pair_info['B'],
                lora_strength
            )
            merged_model[base_key] = merged_weight
            applied_count += 1
            
            if verbose:
                print(f"  ‚úì Applied LoRA to: {base_key}")
                
                # ËÆ°ÁÆóÂèòÂåñÂπÖÂ∫¶
                delta = merged_weight - original_weight
                orig_magnitude = torch.abs(original_weight).mean().item()
                delta_magnitude = torch.abs(delta).mean().item()
                relative_change = delta_magnitude / orig_magnitude if orig_magnitude > 0 else 0
                print(f"    Relative change: {relative_change:.6f}")
        else:
            skipped_count += 1
            if verbose:
                print(f"  ‚ö† Skipped (not in base): {base_key}")
    
    print(f"‚úì Applied LoRA to {applied_count} layers")
    if skipped_count > 0:
        print(f"‚ö† Skipped {skipped_count} layers (not found in base model)")
        
        # ÊòæÁ§∫‰∏Ä‰∫õÊú™ÂåπÈÖçÁöÑÂ±Ç
        missing_keys = [k for k in lora_pairs.keys() if k not in merged_model]
        if missing_keys:
            print(f"üìã Some missing keys (first 5):")
            for key in missing_keys[:5]:
                print(f"  - {key}")
    
    # È™åËØÅËûçÂêàÊïàÊûú
    if applied_count == 0:
        print("\n‚ùå ERROR: No LoRA layers were applied! This suggests a key mismatch.")
        print("üîß Debugging info:")
        
        # ÊòæÁ§∫‰∏Ä‰∫õÂ∫ïÊ®°ÁöÑkeyÁî®‰∫éÂØπÊØî
        base_keys = list(base_model.keys())[:10]
        lora_keys = list(lora_pairs.keys())[:5]
        
        print("üìã Sample base model keys:")
        for key in base_keys:
            print(f"  {key}")
        
        print("üìã Sample LoRA target keys:")
        for key in lora_keys:
            print(f"  {key}")
        
        sys.exit(1)
    
    # ‰øùÂ≠òËûçÂêàÂêéÁöÑÊ®°Âûã
    print(f"\nüíæ Saving merged model...")
    try:
        save_file(merged_model, output_path)
        file_size = os.path.getsize(output_path) / (1024**3)  # GB
        print(f"‚úì Model saved successfully: {output_path}")
        print(f"‚úì File size: {file_size:.2f} GB")
    except Exception as e:
        print(f"Error saving model: {e}")
        sys.exit(1)
    
    # ËÆ°ÁÆóÊï¥‰ΩìÂèòÂåñÁªüËÆ°
    total_params = sum(tensor.numel() for tensor in merged_model.values())
    print(f"\nüìä Merge Statistics:")
    print(f"  - Total parameters: {total_params:,}")
    print(f"  - LoRA layers applied: {applied_count}")
    print(f"  - Coverage: {applied_count/len(base_model)*100:.1f}% of base model layers")
    
    match_rate = applied_count / len(lora_pairs) if len(lora_pairs) > 0 else 0
    print(f"  - LoRA utilization: {match_rate:.1%}")
    
    if match_rate >= 0.8:
        print("\nüéâ SUCCESS: LoRA merged successfully!")
    elif match_rate >= 0.5:
        print(f"\n‚ö†Ô∏è PARTIAL SUCCESS: Only {match_rate:.1%} of LoRA layers were applied.")
        print("   The model should still show some LoRA effects, but may not be optimal.")
    else:
        print(f"\n‚ùå LOW SUCCESS: Only {match_rate:.1%} of LoRA layers were applied.")
        print("   The LoRA effects may be very weak or unnoticeable.")
    
    return merged_model

def main():
    parser = argparse.ArgumentParser(
        description='NextDiT LoRA Merger - Support for lora_A/lora_B format',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  python nextdit_lora_merger_AB.py --base base_model.safetensors --lora my_lora.safetensors --output merged_model.safetensors
  python nextdit_lora_merger_AB.py -b base.safetensors -l lora.safetensors -o merged.safetensors --strength 2.0 --verbose
        '''
    )
    
    parser.add_argument('--base', '-b', required=True,
                        help='Path to base model (.safetensors)')
    parser.add_argument('--lora', '-l', required=True,
                        help='Path to LoRA model (.safetensors)')
    parser.add_argument('--output', '-o', required=True,
                        help='Output path for merged model (.safetensors)')
    parser.add_argument('--strength', '-s', type=float, default=1.0,
                        help='LoRA strength (default: 1.0)')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='Enable verbose output')
    
    args = parser.parse_args()
    
    # È™åËØÅÂèÇÊï∞
    if args.strength < 0 or args.strength > 5:
        print("Warning: LoRA strength outside normal range (0-5)")
    
    # ÊâßË°åÂêàÂπ∂
    try:
        merge_lora_A_B_with_base_model(
            args.base,
            args.lora,
            args.output,
            args.strength,
            args.verbose
        )
    except KeyboardInterrupt:
        print("\n\n‚ö† Operation cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
